Exploring Multi-label Data Augmentation for LLM Fine-tuning and Inference in Requirement Engineering: A Study with Domain Expert Evaluation

Abstract - The application of Large Language Models (LLMs) has brought significant advancements to the field of requirement engineering, offering automated solutions for a variety of engineering tasks. However, these models still face challenges in effectively processing complex and confidential multi-label data. To overcome these challenges, domain-specific data augmentation techniques can be utilized to enhance model performance. In this study, we investigate the impact of multi-label data augmentation on the performance of LLMs during fine-tuning and inference in the context of requirement engineering. We introduce a novel augmentation technique specifically designed for multi-label technical data and assess its effectiveness through a series of controlled experiments. Our approach involves converting multi-label datasets into one-to-one mapping datasets, fine-tuning a seven-billion-parameter decoder-only LLM on these datasets, and conducting thorough performance evaluations on three fine-tuned LLMs using clearly defined metrics, with assessments conducted by five domain experts and three state-of-the-art LLMs. The results show that the proposed data augmentation technique can either effectively enhance or negatively impact the performance of LLMs, contingent on the specific implementation employed. We hope our research provides valuable insights for developing more robust and efficient LLM-based frameworks in various engineering domains.
